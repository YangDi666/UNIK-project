<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
    	<meta name="description" content="UNIK: A Unified Framework for Real-world Skeleton-based Action Recognition">

    	<title>UNIK: A Unified Framework for Real-world Skeleton-based Action Recognition</title>
		
    	<link href="css/bootstrap.min.css" rel="stylesheet">
    	<link href="css/style.css" rel="stylesheet">
    </head>

    <body>

	<div class="container">
      <div class="header">
        <h2><center> 
		A Unified Framework for Real-world Skeleton-based Action Recognition
		<br/> (BMVC'2021 Oral Presentation)</center></h2>
		<h3><center></center></h3>
		<h4> <center> <a href="https://www.linkedin.com/in/di-yang-38479a174/" class="text-info"> Di Yang* </a>  &nbsp&nbsp&nbsp&nbsp  <a href="http://www-sop.inria.fr/members/Yaohui.Wang" class="text-info"> Yaohui Wang* </a>  &nbsp&nbsp&nbsp&nbsp <a href="http://antitza.com" class="text-info">Antitza Dantcheva</a> &nbsp&nbsp&nbsp&nbsp <a href="https://www.linkedin.com/in/lorenzo-garattoni/" class="text-info">Lorenzo Garattoni</a> &nbsp&nbsp&nbsp&nbsp <a href="https://iridia.ulb.ac.be/~gfrancesca/Gianpiero_Francesca/Home.html" class="text-info">Gianpiero Francesca </a> &nbsp&nbsp&nbsp&nbsp <a href="http://www-sop.inria.fr/members/Francois.Bremond/" class="text-info">François Brémond</a> </center> </h4>
		<h4> <center>Inria, &nbsp&nbsp Université Côte d&#39Azur, &nbsp&nbsp Toyota Motor Europe</center> </h4>
	  </div>

	<div class="row">
    <hr>
    <h2><center>Abstract</center></h2>
    <h4>In this work, we introduce UNIK, a novel topology-free skeleton-based action recognition method that is not only effective to learn spatio-temporal features on human skeleton sequences but also able to generalize across datasets. This is achieved by learning an optimal dependency matrix from the uniform distribution based on a multi-head attention mechanism. 
	Moreover, to study the cross-domain generalizability of skeleton-based action recognition in real-world videos, we re-evaluate state-of-the-art approaches as well as the proposed UNIK in light of a novel Posetics dataset. This dataset is created from Kinetics-400 videos by estimating, refining and filtering poses. We provide an analysis on how much performance improves on the smaller benchmark datasets after pre-training on Posetics for the action classification task. Experimental results show that the proposed UNIK, with pre-training on Posetics, generalizes well and outperforms state-of-the-art when transferred onto four target action classification datasets: Toyota Smarthome, Penn Action, NTU-RGB+D 60 and NTU-RGB+D 120.	
    </h4>
	
   <center>
   <h3><a href="https://arxiv.org/pdf/2107.08580" class="text-info">[Paper]</a> &nbsp&nbsp&nbsp&nbsp <a href="https://github.com/YangDi666/UNIK" class="text-info">[Code]</a> &nbsp&nbsp&nbsp&nbsp <a href="https://github.com/YangDi666/UNIK" class="text-info">[Posetics Dataset]</a>
 	<video width="1120" height="630" autoplay="autoplay" loop="loop" muted="muted" controls>
		<source src="demos/0014-video.mp4" type="video/mp4">
	</video>
	
	</center>
	</div>	
	
	
    <div class="row">
    <hr>
    <h2><center>Results</center></h2>
	 
	<figure>
		<center>
		<img src="imgs/curve.jpg" title="" style="max-width:100%;vertical-align:top"/>
		
		</center>
		<!--
		<figcaption><b>UNIK.</b> 
		
	  	-->
		</figure>	
	</div>	
	<hr>
	</div>
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    </body>
</html>
